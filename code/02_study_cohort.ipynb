{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\n\n# Load site config\nconfig_path = os.path.join('..', 'config.json')\nif not os.path.exists(config_path):\n    config_path = os.path.join('..', 'clif_config.json')\nwith open(config_path) as f:\n    config = json.load(f)\nSITE = config.get('site', 'unknown')\n\n#Create structure for consort data information\nconsort = { \n    #initial data information\n    \"total_rows_loaded\": None,\n    \"total_admissions\": None,\n\n    #inclusion/exclusion criteria\n    \"total_nippv_6h\": None,\n    \"total_fio2_60\": None,\n    \"total_pco2_45\": None,\n    \"total_ph_7.35\": None,\n\n    #cohort size and failures BEFORE dropping missing data\n    \"patients_pre_missing\": None,\n    \"failures_pre_missing\": None,\n    \"imv_fail_pre_missing\": None,\n    \"death_fail_pre_missing\": None,\n    \"both_fail_pre_missing\": None,\n\n    #cohort size and failures AFTER dropping missing data\n    \"patients_post_missing\": None,\n    \"failures_post_missing\": None,\n    \"imv_fail_post_missing\": None,\n    \"death_fail_post_missing\": None,\n    \"both_fail_post_missing\": None,\n}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\n\n# import dataset\ndf = pd.read_parquet('../output/study_cohort_NIPPV_&_ICU.parquet')\n\nprint(f'Total rows loaded: {len(df)}')\nconsort[\"total_rows_loaded\"] = len(df)\n\nadmissions = df['hospitalization_id'].nunique()\nprint(f'Total ICU NIPPV Admissions: {admissions}')\nconsort[\"total_admissions\"] = admissions"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to standard pandas datetime\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], utc = True)\n",
    "df['admission_dttm'] = pd.to_datetime(df['admission_dttm'], utc = True)\n",
    "\n",
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 310574\n",
      "Total NIPPV < 6 hrs: 1508\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "vital_sign_cols = ['heart_rate', 'respiratory_rate', 'spo2', 'sbp', 'temp_c']\n",
    "\n",
    "# Identify First Vital Sign\n",
    "vitals = df[df[vital_sign_cols].notna().any(axis=1)]\n",
    "vitals = vitals.sort_values(['hospitalization_id', 'event_time'])\n",
    "\n",
    "first_vital = (\n",
    "    vitals.groupby('hospitalization_id')\n",
    "          .first()\n",
    "          .reset_index()[['hospitalization_id', 'event_time']]\n",
    "          .rename(columns={'event_time': 'first_vital_time'})\n",
    ")\n",
    "\n",
    "# Identify First NIPPV Instance\n",
    "nippv = df[df['device_category'] == 'NIPPV']\n",
    "nippv = nippv.sort_values(['hospitalization_id', 'event_time'])\n",
    "\n",
    "first_nippv = (\n",
    "    nippv.groupby('hospitalization_id')\n",
    "         .first()\n",
    "         .reset_index()[['hospitalization_id', 'event_time']]\n",
    "         .rename(columns={'event_time': 'first_nippv_time'})\n",
    ")\n",
    "\n",
    "# Merge First NIPPV Instance with First Vital and Calculate Time differene (time_to_NIPPV)\n",
    "merged = first_vital.merge(first_nippv, on='hospitalization_id')\n",
    "merged['time_to_NIPPV'] = (\n",
    "    merged['first_nippv_time'] - merged['first_vital_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Filter dataframe to contain only hospitalizations where NIPPV occured within 6 hours of first vital sign\n",
    "eligible_ids = merged[merged['time_to_NIPPV'] <= 6]['hospitalization_id']\n",
    "df = df[df['hospitalization_id'].isin(eligible_ids)]\n",
    "\n",
    "# Merge time_to_NIPPV and first_vital_time back into main dataframe\n",
    "df = df.merge(\n",
    "    merged[['hospitalization_id', 'time_to_NIPPV']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "df = df.merge(\n",
    "    first_vital[['hospitalization_id', 'first_vital_time']],\n",
    "    on='hospitalization_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Print total rows and admissions\n",
    "print(f'Total rows loaded: {len(df)}')\n",
    "admissions = df['hospitalization_id'].nunique()\n",
    "print(f'Total NIPPV < 6 hrs: {admissions}')\n",
    "consort[\"total_nippv_6h\"] = admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows loaded: 126892\n",
      "Total NIPPV < 6 hrs: 1508\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Calculate nippv_start_time\n",
    "df['nippv_start_time'] = (\n",
    "    df['first_vital_time'] + pd.to_timedelta(df['time_to_NIPPV'], unit='h')\n",
    ")\n",
    "\n",
    "#Calculate time_since_nippv\n",
    "df['time_since_nippv'] = (\n",
    "    df['event_time'] - df['nippv_start_time']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "#Filter to rows w/in 48 hours of NIPPV start\n",
    "df = df[df['time_since_nippv'] <= 48]\n",
    "\n",
    "# Print total rows and admissions\n",
    "print(f'Total rows loaded: {len(df)}')\n",
    "admissions = df['hospitalization_id'].nunique()\n",
    "print(f'Total NIPPV < 6 hrs: {admissions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NIPPV < 6 hrs & fio2_set <= 60: 622\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Filter to 1 hour after NIPPV initiation\n",
    "df_PreNIPPV = df[df['event_time'] < df['nippv_start_time'] + pd.Timedelta(hours=1)]\n",
    "\n",
    "# Get the median fio2_set for each hospitalization_id prior to NIPPV initiation\n",
    "median_fio2_PreNIPPV = df_PreNIPPV.groupby('hospitalization_id')['fio2_set'].median().reset_index()\n",
    "\n",
    "# Identify eligible hospitalizations where max fio2_set <= .6 prior to NIPPV initiation\n",
    "eligible_fio2 = median_fio2_PreNIPPV[median_fio2_PreNIPPV['fio2_set'] <= .6]['hospitalization_id']\n",
    "\n",
    "# Filter the df to keep only eligible fio2 hospitalizations\n",
    "df = df[df['hospitalization_id'].isin(eligible_fio2)].reset_index(drop=True)\n",
    "\n",
    "#Print total admissions\n",
    "admissions = df['hospitalization_id'].nunique()\n",
    "print(f'Total NIPPV < 6 hrs & fio2_set <= 60: {admissions}')\n",
    "consort[\"total_fio2_60\"] = admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NIPPV < 6 hrs & fio2_set <= 60 & pco2 >= 45: 264\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Update df_PreNIPPV with only filtered admissions\n",
    "df_PreNIPPV = df[df['event_time'] < df['nippv_start_time'] + pd.Timedelta(hours=1)]\n",
    "\n",
    "# Get median pco2_arterial and pco2_venous prior to NIPPV initiation for each hospitalization_id\n",
    "median_pco2_arterial = df_PreNIPPV.groupby('hospitalization_id')['pco2_arterial'].median()\n",
    "median_pco2_venous = df_PreNIPPV.groupby('hospitalization_id')['pco2_venous'].median()\n",
    "\n",
    "# Combine median_pco2_arterial and median_pco2_venous into one dataframe\n",
    "median_pco2 = pd.DataFrame({\n",
    "    'pco2_arterial': median_pco2_arterial,\n",
    "    'pco2_venous': median_pco2_venous\n",
    "}).reset_index()\n",
    "\n",
    "# Filter rows where median_pco2_arterial or median_pco2_venous >= 45\n",
    "eligible_pco2 = median_pco2[\n",
    "    (median_pco2['pco2_arterial'] >= 45) | (median_pco2['pco2_venous'] >= 45)\n",
    "]['hospitalization_id']\n",
    "\n",
    "# Filter the df to keep only eligible pco2 hospitalizations\n",
    "df = df[df['hospitalization_id'].isin(eligible_pco2)].reset_index(drop=True)\n",
    "admissions = df['hospitalization_id'].nunique()\n",
    "\n",
    "#Print total admissions\n",
    "print(f'Total NIPPV < 6 hrs & fio2_set <= 60 & pco2 >= 45: {len(eligible_pco2)}')\n",
    "consort[\"total_pco2_45\"] = len(eligible_pco2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NIPPV < 6 hrs & fio2_set <= 60 & pco2 >= 45 & ph <= 7.35: 207\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "#Update df_PreNIPPV with only filtered admissions\n",
    "df_PreNIPPV = df[df['event_time'] < df['nippv_start_time'] + pd.Timedelta(hours=1)]\n",
    "\n",
    "# Get median ph_arterial and ph_venous prior to NIPPV initiation for each hospitalization_id\n",
    "median_ph_arterial = df_PreNIPPV.groupby('hospitalization_id')['ph_arterial'].median()\n",
    "median_ph_venous = df_PreNIPPV.groupby('hospitalization_id')['ph_venous'].median()\n",
    "\n",
    "# Combine median_ph_arterial and median_ph_venous into one dataframe\n",
    "median_ph = pd.DataFrame({\n",
    "    'ph_arterial': median_ph_arterial,\n",
    "    'ph_venous': median_ph_venous\n",
    "}).reset_index()\n",
    "\n",
    "# Filter to rows where median_ph_arterial or median_ph_venous <= 7.35\n",
    "eligible_ph = median_ph[\n",
    "    (median_ph['ph_arterial'] <= 7.35) | (median_ph['ph_venous'] <= 7.35)\n",
    "]['hospitalization_id']\n",
    "\n",
    "# Filter the df to keep only eligible ph hospitalizations\n",
    "df = df[df['hospitalization_id'].isin(eligible_ph)].reset_index(drop=True)\n",
    "admissions = df['hospitalization_id'].nunique()\n",
    "\n",
    "#Print total admissions\n",
    "print(f'Total NIPPV < 6 hrs & fio2_set <= 60 & pco2 >= 45 & ph <= 7.35: {len(eligible_ph)}')\n",
    "consort[\"total_ph_7.35\"] = admissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients: 207\n",
      "Total failures: 69\n",
      "Total IMV failures: 38\n",
      "Total Death failures: 23\n",
      "Both failures: 8\n"
     ]
    }
   ],
   "source": [
    "# Sort by hospitalization_id and reset index after sorting\n",
    "df = df.sort_values(by = ['hospitalization_id', 'event_time'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Create a new dataframe of IMV events\n",
    "imv_df = df[df['device_category'] == 'IMV'].copy()\n",
    "# Identify IMV hospitalizations\n",
    "imv_ids = imv_df['hospitalization_id'].unique()\n",
    "\n",
    "# Create a new dataframe of patients who died\n",
    "expired_df = df[df['discharge_category'] == 'Expired'].copy()\n",
    "# Identify deaths\n",
    "expired_ids = expired_df['hospitalization_id'].unique()\n",
    "\n",
    "# Create separate failure flags\n",
    "df['failure_imv'] = df['hospitalization_id'].isin(imv_ids).astype(int)\n",
    "df['failure_death'] = df['hospitalization_id'].isin(expired_ids).astype(int)\n",
    "\n",
    "# Overall failure: either IMV or death\n",
    "df['failure'] = ((df['failure_imv'] == 1) | (df['failure_death'] == 1)).astype(int)\n",
    "\n",
    "# Count totals (unique hospitalizations)\n",
    "total_patients = df['hospitalization_id'].nunique()\n",
    "total_failures = df[df['failure'] == 1]['hospitalization_id'].nunique()\n",
    "imv_failures = df[df['failure_imv'] == 1]['hospitalization_id'].nunique()\n",
    "death_failures = df[df['failure_death'] == 1]['hospitalization_id'].nunique()\n",
    "both_failures = df[(df['failure_imv'] == 1) & (df['failure_death'] == 1)]['hospitalization_id'].nunique()\n",
    "\n",
    "#Print total patients and failures\n",
    "print(f'Total patients: {total_patients}')\n",
    "consort[\"patients_pre_missing\"] = total_patients\n",
    "print(f'Total failures: {total_failures}')\n",
    "consort[\"failures_pre_missing\"] = total_failures\n",
    "print(f'Total IMV failures: {imv_failures - both_failures}')\n",
    "consort[\"imv_fail_pre_missing\"] = imv_failures - both_failures\n",
    "print(f'Total Death failures: {death_failures - both_failures}')\n",
    "consort[\"death_fail_pre_missing\"] = death_failures - both_failures\n",
    "print(f'Both failures: {both_failures}')\n",
    "consort[\"both_fail_pre_missing\"] = both_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter events 1–12 hours after NIPPV\n",
    "df_PostNIPPV_window = df[(df['time_since_nippv'] >= 1) & (df['time_since_nippv'] <= 12)]\n",
    "\n",
    "# Sort to ensure earliest events first\n",
    "df_PostNIPPV_window = df_PostNIPPV_window.sort_values(['hospitalization_id', 'event_time'])\n",
    "\n",
    "# Get the median heart rate within the 1–12 hour window\n",
    "heart_rate_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['heart_rate']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'heart_rate': 'heart_rate_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(heart_rate_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the median respiratory rate within the 1-12 hour window\n",
    "resp_rate_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['respiratory_rate']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'respiratory_rate': 'respiratory_rate_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(resp_rate_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arterial and venous PCO2\n",
    "df_PostNIPPV_window['PostNIPPV_pco2_combined'] = df_PostNIPPV_window['pco2_arterial'].combine_first(df_PostNIPPV_window['pco2_venous'])\n",
    "\n",
    "# Get median combined PCO2 in window\n",
    "pco2_PostNIPPV_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['PostNIPPV_pco2_combined']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'PostNIPPV_pco2_combined': 'pco2_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back\n",
    "df = df.merge(pco2_PostNIPPV_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine arterial and venous pH\n",
    "df_PostNIPPV_window['PostNIPPV_ph_combined'] = df_PostNIPPV_window['ph_arterial'].combine_first(df_PostNIPPV_window['ph_venous'])\n",
    "\n",
    "# Get median combined pH in window\n",
    "ph_PostNIPPV_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['PostNIPPV_ph_combined']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'PostNIPPV_ph_combined': 'ph_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back into main df\n",
    "df = df.merge(ph_PostNIPPV_window, on='hospitalization_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the median peep_set within the 1-12 hour window\n",
    "peep_set_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['peep_set']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'peep_set': 'peep_set_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(peep_set_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the median tidal_volume_obs within the 1-12 hour window\n",
    "tidal_volume_obs_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['tidal_volume_obs']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'tidal_volume_obs': 'tidal_volume_obs_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(tidal_volume_obs_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the median fio2_set within the 1–12 hour window\n",
    "fio2_PostNIPPV_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['fio2_set']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'fio2_set': 'fio2_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(fio2_PostNIPPV_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the median fio2_set within the 1–12 hour window\n",
    "map_PostNIPPV_window = (\n",
    "    df_PostNIPPV_window.groupby('hospitalization_id')['map']\n",
    "    .median()\n",
    "    .reset_index()\n",
    "    .rename(columns={'map': 'map_after_NIPPV'})\n",
    ")\n",
    "\n",
    "# Merge back to main df\n",
    "df = df.merge(map_PostNIPPV_window, on='hospitalization_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = df.groupby('hospitalization_id').agg({\n",
    "    'age_at_admission': 'first',\n",
    "    'sex_category': 'first',\n",
    "    'map_after_NIPPV': 'first',\n",
    "    'peep_set_after_NIPPV': 'first',\n",
    "    'tidal_volume_obs_after_NIPPV': 'first',\n",
    "    'heart_rate_after_NIPPV': 'first',\n",
    "    'respiratory_rate_after_NIPPV': 'first',\n",
    "    'ph_after_NIPPV': 'first',\n",
    "    'pco2_after_NIPPV': 'first',\n",
    "    'fio2_after_NIPPV': 'first',\n",
    "    'failure_imv':'first',\n",
    "    'failure_death':'first',\n",
    "    'failure': 'first'\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =====================================================\n# MISSINGNESS TABLE (TRIPOD+AI + STROBE required)\n# Generated BEFORE complete case deletion\n# =====================================================\n\npredictor_cols = [\n    'age_at_admission', 'sex_category', 'map_after_NIPPV',\n    'peep_set_after_NIPPV', 'tidal_volume_obs_after_NIPPV',\n    'heart_rate_after_NIPPV', 'respiratory_rate_after_NIPPV',\n    'ph_after_NIPPV', 'pco2_after_NIPPV', 'fio2_after_NIPPV'\n]\n\nmissingness_rows = []\nn_total = len(df_analytic)\nfor col in predictor_cols:\n    n_missing = int(df_analytic[col].isna().sum())\n    missingness_rows.append({\n        'variable': col,\n        'N_total': n_total,\n        'N_missing': n_missing,\n        'N_observed': n_total - n_missing,\n        'Pct_missing': round(100 * n_missing / n_total, 1) if n_total > 0 else 0\n    })\n\n# Also add the outcome variable\nn_fail_missing = int(df_analytic['failure'].isna().sum())\nmissingness_rows.append({\n    'variable': 'failure',\n    'N_total': n_total,\n    'N_missing': n_fail_missing,\n    'N_observed': n_total - n_fail_missing,\n    'Pct_missing': round(100 * n_fail_missing / n_total, 1) if n_total > 0 else 0\n})\n\nmissingness_df = pd.DataFrame(missingness_rows)\nmissingness_df['site'] = SITE\n\nos.makedirs('../output_to_share', exist_ok=True)\nmissingness_df.to_csv('../output_to_share/missingness_table.csv', index=False)\n\nprint(f\"Missingness table exported for site: {SITE}\")\nprint(missingness_df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing values\n",
    "df_analytic_clean = df_analytic.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patients before dropping missing data: 207\n",
      "Total patients after dropping missing data: 95\n"
     ]
    }
   ],
   "source": [
    "#Print total number of patients before dropping missing data\n",
    "admissions = df_analytic['hospitalization_id'].nunique()\n",
    "print(f'Total patients before dropping missing data: {admissions}')\n",
    "\n",
    "#Print total number of patients after dropping missing data\n",
    "admissions = df_analytic_clean['hospitalization_id'].nunique()\n",
    "print(f'Total patients after dropping missing data: {admissions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have a copy to avoid warnings\n",
    "df_analytic_clean = df_analytic_clean.copy()\n",
    "\n",
    "# Scale continuous variables\n",
    "df_analytic_clean.loc[:, 'age_scale'] = (df_analytic_clean['age_at_admission'] - df_analytic_clean['age_at_admission'].mean()) / 10\n",
    "df_analytic_clean.loc[:, 'pco2_scale'] = (df_analytic_clean['pco2_after_NIPPV'] - df_analytic_clean['pco2_after_NIPPV'].mean()) / 10\n",
    "df_analytic_clean.loc[:, 'ph_scale'] = (df_analytic_clean['ph_after_NIPPV'] - df_analytic_clean['ph_after_NIPPV'].mean()) / 0.1\n",
    "df_analytic_clean.loc[:, 'rr_scale'] = (df_analytic_clean['respiratory_rate_after_NIPPV'] - df_analytic_clean['respiratory_rate_after_NIPPV'].mean()) / 5\n",
    "df_analytic_clean.loc[:, 'hr_scale'] = (df_analytic_clean['heart_rate_after_NIPPV'] - df_analytic_clean['heart_rate_after_NIPPV'].mean()) / 10\n",
    "df_analytic_clean.loc[:, 'tidal_volume_scale'] = (df_analytic_clean['tidal_volume_obs_after_NIPPV'] - df_analytic_clean['tidal_volume_obs_after_NIPPV'].mean()) / 100\n",
    "df_analytic_clean['peep_scale'] = (df_analytic_clean['peep_set_after_NIPPV'] - df_analytic_clean['peep_set_after_NIPPV'].mean()) / 2\n",
    "df_analytic_clean.loc[:, 'map_scale'] = (df_analytic_clean['map_after_NIPPV'] - df_analytic_clean['map_after_NIPPV'].mean()) / 10\n",
    "\n",
    "\n",
    "# Binary variables\n",
    "df_analytic_clean.loc[:, 'female'] = (df_analytic_clean['sex_category'] == 'Female').astype(int)\n",
    "df_analytic_clean.loc[:, 'fio2_high'] = (df_analytic_clean['fio2_after_NIPPV'] > 0.40).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export final df to CSV\ndf_analytic_clean.to_csv('../output/NIPPV_analytic_dataset.csv', index=False)"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patients: 95\n",
      "Total failures: 43\n",
      "Total IMV failures: 29\n",
      "Total death failures: 6\n",
      "Both failures: 8\n"
     ]
    }
   ],
   "source": [
    "# Total number of rows\n",
    "total_rows = len(df_analytic_clean)\n",
    "\n",
    "# Total number of failures (failure == 1)\n",
    "total_failures = df_analytic_clean['failure'].sum()\n",
    "imv_failures = df_analytic_clean['failure_imv'].sum()\n",
    "death_failures = df_analytic_clean['failure_death'].sum()\n",
    "both_failures = df_analytic_clean[(df_analytic_clean['failure_imv'] == 1) & (df_analytic_clean['failure_death'] == 1)]['hospitalization_id'].nunique()\n",
    "\n",
    "#Print total patients and failures\n",
    "print(f'Total Patients: {total_rows}')\n",
    "consort[\"patients_post_missing\"] = total_rows\n",
    "print(f'Total failures: {total_failures}')\n",
    "consort[\"failures_post_missing\"] = total_failures\n",
    "print(f'Total IMV failures: {imv_failures - both_failures}')\n",
    "consort[\"imv_fail_post_missing\"] = imv_failures - both_failures\n",
    "print(f'Total death failures: {death_failures - both_failures}')\n",
    "consort[\"death_fail_post_missing\"] = death_failures - both_failures\n",
    "print(f'Both failures: {both_failures}')\n",
    "consort[\"both_fail_post_missing\"] = both_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# =====================================================\n# EXPORT CONSORT DATA — Structured for Flow Diagram\n# =====================================================\nos.makedirs('../output_to_share', exist_ok=True)\n\n# Original flat format (backward compatible)\nconsort_flat = pd.DataFrame([consort])\nconsort_flat['site'] = SITE\nconsort_flat.to_csv('../output_to_share/consort.csv', index=False)\n\n# Structured flow diagram format (for manuscript Figure 1)\nflow_steps = [\n    {\n        'step': 1,\n        'description': 'Total ICU admissions loaded',\n        'n_remaining': consort['total_admissions'],\n        'n_excluded': None,\n        'exclusion_reason': None\n    },\n    {\n        'step': 2,\n        'description': 'NIPPV initiated within 6h of first vital sign',\n        'n_remaining': consort['total_nippv_6h'],\n        'n_excluded': consort['total_admissions'] - consort['total_nippv_6h'],\n        'exclusion_reason': 'No NIPPV within 6 hours of first vital sign'\n    },\n    {\n        'step': 3,\n        'description': 'Baseline FiO2 <= 60%',\n        'n_remaining': consort['total_fio2_60'],\n        'n_excluded': consort['total_nippv_6h'] - consort['total_fio2_60'],\n        'exclusion_reason': 'Baseline FiO2 > 60% (possible hypoxemic respiratory failure)'\n    },\n    {\n        'step': 4,\n        'description': 'Baseline pCO2 >= 45 mmHg',\n        'n_remaining': consort['total_pco2_45'],\n        'n_excluded': consort['total_fio2_60'] - consort['total_pco2_45'],\n        'exclusion_reason': 'pCO2 < 45 mmHg (no hypercapnia)'\n    },\n    {\n        'step': 5,\n        'description': 'Baseline pH <= 7.35',\n        'n_remaining': consort['total_ph_7.35'],\n        'n_excluded': consort['total_pco2_45'] - consort['total_ph_7.35'],\n        'exclusion_reason': 'pH > 7.35 (no respiratory acidosis)'\n    },\n    {\n        'step': 6,\n        'description': 'Complete case analysis (no missing predictors)',\n        'n_remaining': consort['patients_post_missing'],\n        'n_excluded': consort['patients_pre_missing'] - consort['patients_post_missing'],\n        'exclusion_reason': 'Missing predictor data'\n    }\n]\n\nconsort_flow = pd.DataFrame(flow_steps)\nconsort_flow['site'] = SITE\nconsort_flow['n_failure_yes'] = None\nconsort_flow['n_failure_no'] = None\n\n# Fill in failure breakdown for final step\nconsort_flow.loc[consort_flow['step'] == 6, 'n_failure_yes'] = consort['failures_post_missing']\nconsort_flow.loc[consort_flow['step'] == 6, 'n_failure_no'] = (\n    consort['patients_post_missing'] - consort['failures_post_missing']\n)\n\nconsort_flow.to_csv('../output_to_share/consort_flow.csv', index=False)\n\nprint(f\"CONSORT data exported for site: {SITE}\")\nprint(consort_flow.to_string(index=False))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}